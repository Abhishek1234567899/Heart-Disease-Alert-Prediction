{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.logger.logging import logging\n",
    "from src.exception.exception import customexception\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "\n",
    "from src.utils.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig()\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_data_transformation(self):\n",
    "        \n",
    "        try:\n",
    "            logging.info('Data Transformation initiated')\n",
    "            \n",
    "            '''# Define which columns should be ordinal-encoded and which should be scaled\n",
    "            categorical_cols = ['cut', 'color','clarity']\n",
    "            numerical_cols = ['carat', 'depth','table', 'x', 'y', 'z']\n",
    "            \n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories = ['Fair', 'Good', 'Very Good','Premium','Ideal']\n",
    "            color_categories = ['D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "            clarity_categories = ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']'''\n",
    "            \n",
    "            logging.info('Pipeline Initiated')\n",
    "            \n",
    "            ## Numerical Pipeline\n",
    "            num_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='median')),\n",
    "                ('scaler',StandardScaler())\n",
    "\n",
    "                ]\n",
    "\n",
    "            )\n",
    "            \n",
    "            ''''# Categorigal Pipeline\n",
    "            cat_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\n",
    "                ('scaler',StandardScaler())\n",
    "                ] )'''\n",
    "\n",
    "        \n",
    "            \n",
    "            preprocessor=ColumnTransformer([\n",
    "            ('num_pipeline',num_pipeline,numerical_cols)])\n",
    "           # , ('cat_pipeline',cat_pipeline,categorical_cols)\n",
    "            #])\n",
    "            \n",
    "            return preprocessor\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise customexception(e,sys)\n",
    "            \n",
    "    \n",
    "    def initialize_data_transformation(self,train_path,test_path):  \n",
    "        try:\n",
    "            train_df=pd.read_csv(train_path)\n",
    "            test_df=pd.read_csv(test_path)\n",
    "            \n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train Dataframe Head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test Dataframe Head : \\n{test_df.head().to_string()}')\n",
    "            \n",
    "            preprocessing_obj = self.get_data_transformation()\n",
    "            \n",
    "            target_column_name = 'target'\n",
    "            drop_columns = [target_column_name,'id']\n",
    "            \n",
    "          #  input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_train_df=train_df[target_column_name] \n",
    "            \n",
    "            \n",
    "            input_feature_test_df=test_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_test_df=test_df[target_column_name]\n",
    "            \n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            \n",
    "            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\n",
    "            \n",
    "            logging.info(\"Applying preprocessing object on training and testing datasets.\")\n",
    "            \n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            logging.info(\"preprocessing pickle file saved\")\n",
    "            \n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.logger.logging import logging\n",
    "from src.exception.exception import customexception\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from src.utils.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig()\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        try:\n",
    "            logging.info('Data Transformation initiated')\n",
    "            \n",
    "            # Define numerical columns\n",
    "            numerical_cols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                              'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "            \n",
    "            logging.info('Pipeline Initiated')\n",
    "            \n",
    "            # Numerical Pipeline\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            preprocessor = num_pipeline\n",
    "            \n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occurred in the initiate_data_transformation\")\n",
    "            raise customexception(e, sys)\n",
    "\n",
    "    def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            logging.info(\"Read train and test data complete\")\n",
    "            logging.info(f'Train Dataframe Head:\\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test Dataframe Head:\\n{test_df.head().to_string()}')\n",
    "            \n",
    "            preprocessing_obj = self.get_data_transformation()\n",
    "            \n",
    "            target_column_name = 'target'\n",
    "            drop_columns = [target_column_name]\n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns=drop_columns, axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            \n",
    "            input_feature_test_df = test_df.drop(columns=drop_columns, axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "            \n",
    "            logging.info(\"Applying preprocessing object on training and testing datasets.\")\n",
    "            \n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            \n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Preprocessing pickle file saved\")\n",
    "            \n",
    "            return train_arr, test_arr\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occurred in the initialize_data_transformation\")\n",
    "            raise customexception(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.logger.logging import logging\n",
    "from src.exception.exception import customexception\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
